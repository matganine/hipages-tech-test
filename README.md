# hipages Data Engineer Tech Challenge

The aim of this challenge is to write an ETL program to read json data 
and perform a series of transformations on it to build a structured table 
as a result.

## Instructions for executing the code

Installing dependencies:
```
pip install -r requirements.txt
```
Running the main program:
```
python -m hipages.etl
```
This will generate the two required structured tables as csv files 
in the hipages/output folder: `user_activities.csv` and 
`agg_events.csv`

Running the unit tests:
```
python -m unittest discover -v
```

## Assumptions

The goal of this exercise is to build an ETL process that reads json data,
performs a series of transformations on it to build two structured tables
and write those tables into a csv.

For the purpose of this exercise, I have made the following hypotheses:
- The source json file is available locally.
- It isn't a very large file, so it fits into memory.
- The ETL process needs to be able to cope with source files with 
  anomalies. The data source file is a json lines file, following 
  the JSON schema source file. If the file is corrupted, or the file path
  is wrong, our process will raise an error. 
  If on the other hand the file can be read, but a JSON document doesn't
  follow the correct JSON schema, it will be ignored by our program, and 
  our program will continue processing the other JSON documents inside
  the source file.
- The two output structured tables need to be written locally on a csv 
  file
  
## Notes

While validating the source data using the schema file, I noticed that
the type of the user id property was string while it was an integer 
in the source data file, so I changed the type of this property to 
integer in the schema file.

Regarding the second structured output table (the aggregated table),
the schema required was different in the specification table and in the 
example below. In fact, the specification only requires 5 fields 
(time_bucket, url_level1, activity, activity_count, user_count) while 
the example result includes the url_level2 field as well. So in my ETL
program, I made the assumption that the field url_level2 was also 
required in the aggregated structured table.

## Structure of the code

The structure of the code is pretty straightforward. The ETL program
is in the hipages package. The utils.py file contains helper functions
used to validate data and parse urls. The etl.py file contains the 
ETL program. It is structured in three parts:
- Extractor
  * `extract_data` Responsible for validating and extracting data from
    a data source file and JSON schema file
- Transformers
  * `transform_clean_data` Responsible for transforming the raw dataframe
    into a clean dataframe that can be used to be transformed later on
    into the required structured tables. The clean dataframe contains
    the following columns: event_id, user_session_id, user_id, user_ip, 
    time_stamp, url, url_level1, url_level2, url_level3, activity
  * `transform_data_user_activities` Responsible for transforming the clean 
    dataframe into the user activities dataframe. This transformation
    selects the required fields for the resulting user activities table
  * `transform_data_for_agg_events`  Responsible for transforming the clean
    dataframe into the aggregated dataframe. This transformation 
    aggregates the data in the clean dataframe generated by 
    `transform_clean_data` by creating the time_bucket column and 
    aggregating while grouping by time_bucket, url_level1, url_level2,
    'activity' and counting the number of events and the unique number
    of users
- Loader
  * `load_data` Responsible for loading an output dataframe into a 
    structured table (as a csv file) stored locally

The `etl` function pipes together all those processes to generate the full
ETL process responsible for generating two structured tables based on
a data source file and schema.

Finally, unit tests can be found in the tests package for all the 
functions developed in the hipages package. They allow us to test multiple
use cases in order to make sure that our etl process is able to handle
gracefully source files with anomalies such as schema violation or 
incomplete rows.

## Future iterations, how to make the solution evolve and scale

Our current solution is static and cannot be easily scaled. For instance,
if we have another data source with another schema, we will be able to 
reuse the `extract_data` and `load_data` functions, however we will have
to develop new transformer functions, and if the data source is 
a database, a cloud based distributed file system like S3, or a data 
stream, our `extract_data` won't be reusable. The same goes for 
`load_data` if the sink is something other than a locally stored csv
file.

In order to make our solution more scalable and ready for production,
we would need first to define an ETL abstraction. 

In fact, the Extractor abstraction would allow us to 
extract data from some generic source of data (it could be a database, 
a distributed file system, a Kafka stream...) into a pandas dataframe
for instance, that we could further use to clean, process and aggregate
the data. This extractor would be initialized with the configurations 
of the generic data source.

The Transformer abstraction would allow us to transform the data from a 
given generic type to another generic type. Therefore, piping a list of 
transformers together would let us process the data into the required
end result.

Finally, the Loader abstraction would allow us to load the resulting data
from our transformations into a generic sink (it could be a database, 
a distributed filesystem, a local filesystem...). This Loader would be 
initialized with the configurations of the generic data sink.

As a result, our full ETL process would pipe together an Extractor, 
a list of Transformers and a Loader in order to process the data end 
to end.

## Scaling for big datasets

When dealing with big datasets, we need to make sure that out datasets 
are correctly partitioned (by date for instance with a granularity that
will depend on the amount of data). Hence, those big datasets are 
consumed in small chunks, but we need our infrastructure to be able to 
distributely process lots of those small chunks. In order to do so, 
we have several options. 

We could for instance use Airflow to orchestrate our ETL workflows. Using
the Kubernetes task engine with a Celery executor would allow us to scale
according to the workload. In fact, Airflow's scheduler would run each 
task on a new pod and delete it after completion. Running tasks through
Kubernetes allows us to decouple the orchestration from the execution. 
Hence, the ETL execution code and dependencies are not defined inside
the DAGs. Our underlying ETL tasks are then defined in docker images.

If our datasets are too big to fit in memory for our aggregations for
instance, we could use a distributed computing engine like Spark in order
to scale. That would require rewriting our Extractors, Transformers and
Loaders in Scala or PySpark in order to use Spark datasets to distributely
process the data.

## Bonus

In our ETL process, we have transformed the data in two structured tables:
- User activities which is basically a table with all the user events
  with the timestamp of when the event was fired, the user id, the 
  activity that took place, and the url path parsed into three levels.
- Aggregated events' count and unique user count with the hour 
  granularity 
  
We could imagine another transformation on the source data for exposing
another type of information. In fact, we could use the user IP address
in order to infer a zip code and a city. That would allow us to better
understand the distribution of hipages' users across the country.

We could also filter our events on the urls containing a url_level2 equal
to 'find' and activity equal to 'page_view' in order to compute a ranking 
of the most daily searched tradies' categories (electricians, plumbers, 
painters...) distributed by region. 
This would help us better understand the offer/demand equilibrium for
each region and for each business.

The resulting tables would follow the following schema:

| Field | Description | 
| --- | --- |
| time_bucket | daily or monthly time granularity |
| city | name of the city where the activity occurred|
| zip_code | zipcode where the activity occurred |
| user_count | unique number of users that visited the website in a given city and zip code |

| Field | Description | 
| --- | --- |
| time_bucket | daily or monthly time granularity |
| category | tradies category |
| city | name of the city where the search occurred |
| zip_code | zipcode where the search occurred |
| user_count | unique number of users that searched for a tradie in a given category |
